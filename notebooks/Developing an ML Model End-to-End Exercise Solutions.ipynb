{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmozR9UHC3w6"
   },
   "source": [
    "# Developing an ML Model End-to-End: Exercise Solutions\n",
    "\n",
    "Now that you've had some exposure to using Python for training, tuning, evaluating, and selecting an ML model, it's time to try some exercises on your own. Below, you'll find four exercises that utilize a new training data set, the Diagnostic Breast Cancer Wisconsin data set. You can read more about this data set [here](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)). When applicable, an exercise will begin with some starter code to help guide your development towards a solution to the problem. Remember, there isn't just one solution to the problem and your code may produce the similar results even if it's written differently from what is supplied in the solution notebook.\n",
    "\n",
    "These exercises are intended to be performed on Anaconda Notebooks to avoid the need to download any additional Python libraries besides `ydata_profiler`. If you decide to do these exercises on your own machine, you'll need to download the data set locally as well as any of the Python libraries used. The first code block is provided for you to download the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries needed\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import tarfile\n",
    "\n",
    "# !pip install ydata-profiling\n",
    " \n",
    "# Create a function for pulling U.S. Census data for California housing\n",
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/jsukup/Developing-an-ML-Model-End-to-End/master'\n",
    "DATA_PATH = os.path.join('datasets', 'cancer')\n",
    "DATA_URL = DOWNLOAD_ROOT + 'datasets/cancer/cancer.tgz'\n",
    "\n",
    "def fetch_cancer_data(data_url=DATA_URL, data_path=DATA_PATH):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    tgz_path = os.path.join(data_path, 'cancer.tgz')\n",
    "    urllib.request.urlretrieve(data_url, tgz_path)\n",
    "    data_tgz = tarfile.open(tgz_path)\n",
    "    data_tgz.extractall(path=data_path)\n",
    "    data_tgz.close()\n",
    "\n",
    "fetch_cancer_data() # Pull data set from GitHub\n",
    "\n",
    "# Create a function for loading data into a Python object\n",
    "def load_cancer_data(data_path=DATA_PATH):\n",
    "    csv_path = os.path.join(data_path, 'cancer.csv')\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "data = load_cancer_data()\n",
    "data.head() # Inspect first five rows of data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Describe and Verify Data Quality\n",
    "\n",
    "To begin, have a look at the data set in its raw form before performing any transformations. The easiest approach is the use the `ydata_profiling` library.  Answer the following questions:\n",
    "1. Describe the data set. How many variables/features does it include? What do you believe is the target we would like to predict?\n",
    "2. Are there any missing values?\n",
    "3. Are there any highly correlated variables/features with a correlation coefficient >.7? If so, what are they? Are there any variables/features highly correlated with the target variable that might be good predictors?\n",
    "4. Are there any variables/features we can remove?\n",
    "5. What is the proportion of each label for the target variable/feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data profiler and save the output to an HTML file\n",
    "from ydata_profiling import ProfileReport \n",
    "\n",
    "profile = ProfileReport(data)\n",
    "profile.to_file('profile.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Data Preparation\n",
    "\n",
    "1. Write a function to delete any variables/features that are not useful to training the model (e.g., empty variables/features)\n",
    "2. Write a function to convert the categorical target variable/feature to a binary, numeric one.\n",
    "3. Write a function to apply `StandardScaler` to the training variables/features (but *not* the target variable/feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete any unneeded variables/features\n",
    "def delete_unnamed_32(data):\n",
    "    if 'Unnamed: 32' in data.columns:\n",
    "        return data.drop('Unnamed: 32', axis=1, inplace=True)\n",
    "    else:\n",
    "        return print(\"Variable 'Unnamed: 32' does not exist in the DataFrame.\")\n",
    "\n",
    "delete_unnamed_32(data)\n",
    "\n",
    "# Function to convert the categorical target variable/feature\n",
    "def convert_target_variable(data, target_column):\n",
    "    data[target_column] = data[target_column].map({'B': 0, 'M': 1})\n",
    "    return data\n",
    "\n",
    "convert_target_variable(data, 'diagnosis')\n",
    "\n",
    "# Function to apply `StandardScaler`\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_standard_scaler(data):\n",
    "    scaler = StandardScaler()\n",
    "    data.iloc[:,2:] = scaler.fit_transform(data.iloc[:,2:])\n",
    "    return data\n",
    "\n",
    "apply_standard_scaler(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Model Development\n",
    "\n",
    "1. Split the data set into training data and testing data using the `scikit-learn` function `train_test_split`. You should create 4 new Python objects: `x_train`,`x_test`,`y_train`,`y_test`. This way we'll hold the target variable/feature in it's own object. Use an 80/20 train/test split with the `random_state=734`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/test split\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "y = data['diagnosis']\n",
    "x = data.drop(['id','diagnosis'], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=734) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train up to 3 different models using `scikit-learn` to build a model that predicts the target variable/feature using the training variables/features. Since the target is a discrete value, this will be a classification model. More information on models and `scikit-learn` can be found [here](https://scikit-learn.org/stable/). After training, save each model's prediction on the testing data in a new Python object. You don't have to adjust the default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(x_train, y_train)\n",
    "\n",
    "# Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(x_train, y_train)\n",
    "\n",
    "# K-Nearest Neighbors model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier()\n",
    "sgd_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate each model using a simple measurement of Accuracy (use the `scikit-learn` method `.score()`) comparing the predicted values with the actual values in the `y_test` Python object created earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model accuracy\n",
    "lr_score = lr_clf.score(x_test, y_test)\n",
    "rf_score = rf_clf.score(x_test, y_test)\n",
    "sgd_score = sgd_clf.score(x_test, y_test)\n",
    "\n",
    "print(f\"Model accuracy:\\n Logistic regression: {lr_score},\\n Random Forest: {rf_score},\\n SGD: {sgd_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Hyperparameter Optimization and Model Selection\n",
    "\n",
    "Now that we've trained a few candidate models, we can try and improve their performance using hyperparameter optimization.\n",
    "\n",
    "1. Chose a single model or all the candidate models to perform hyperparameter optimization on. You may use the built-in tools in `scikit-learn` that we used in the course exercises (e.g., `GridSearchCV` and `RandomizedSearchCV`). Remember to choose a `scoring` parameter that's suitable for a classification model like `accuracy`! You should be able to reuse a lot of the same code from the course lab, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model using grid search with cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a list of hyperparameter values set as key:value pairs\n",
    "param_grid = [\n",
    "    # Try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 5, 10, 30], 'max_features': [2, 4, 6, 8, 10]},\n",
    "    # Try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "# Train Random Forest model\n",
    "forest_clf = RandomForestClassifier(random_state=734)\n",
    "\n",
    "# Five fold cross-validation: (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_clf, \n",
    "                           param_grid, \n",
    "                           cv=5,\n",
    "                           scoring='accuracy',\n",
    "                           return_train_score=True)\n",
    "\n",
    "# Fit model to training data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Print best parameters and estimator\n",
    "print(f\"Optimal estimated hyperparameters: {grid_search.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model using grid search with cross-validation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Create a range of hyperparameter values set as key:value pairs\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "}\n",
    "\n",
    "# Train Random Forest model\n",
    "forest_clf = RandomForestClassifier(random_state=734)\n",
    "\n",
    "# Five fold cross-validation: (12+6)*5=90 rounds of training \n",
    "rand_search = RandomizedSearchCV(forest_clf, \n",
    "                                 param_distribs,\n",
    "                                 cv=5,\n",
    "                                 scoring='accuracy',\n",
    "                                 return_train_score=True)\n",
    "\n",
    "# Fit model to training data\n",
    "rand_search.fit(x_train, y_train)\n",
    "\n",
    "# Print best parameters and estimator\n",
    "print(f\"Optimal estimated hyperparameters: {rand_search.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compare the results of the first model(s) to the second model(s) with optimal hyperparameters. You'll probably have different results depending on which technique was used. Which model performs the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model with GridSearch optimal hyperparameters\n",
    "forest_gs = RandomForestClassifier(max_features=8, n_estimators=10, random_state=734)\n",
    "forest_gs.fit(x_train, y_train)\n",
    "forest_gs_score = forest_gs.score(x_test, y_test)\n",
    "\n",
    "# Train a Random Forest model with the RandomSearch optimal hyperparameters\n",
    "forest_rs = RandomForestClassifier(max_features=5, n_estimators=168, random_state=734)\n",
    "forest_rs.fit(x_train, y_train)\n",
    "forest_rs_score = forest_rs.score(x_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Model accuracy:\\n Random Forest (GS): {forest_gs_score},\\n Random Forest (RS): {forest_rs_score},\\n Random Forest (default): {rf_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats! That's the end of the exercises!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "The Machine Learning Pipeline.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
