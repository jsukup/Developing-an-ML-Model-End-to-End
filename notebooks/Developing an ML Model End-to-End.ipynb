{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lsvK85jC3tB"
   },
   "source": [
    "# Developing an ML Model End-to-End\n",
    "Congrats! You are the new data scientist for *The ML Real Estate Company*.  You've been asked to build a machine learning solution to help the company predict median home values across districts in California. The output of this model will be used as input to another model used by executives to choose where to invest in real estate ultimately driving revenue. Currently, the company estimates prices using a group of SMEs and a set of complex rules. Estimates are typically off by nearly 20%. \n",
    "\n",
    "The *U.S. Census Bureau* publishes data on median home value along with other useful features for each district (or *block group* to be technically correct) in the United States. You will build a machine learning *pipeline* to pass data through the stages of the **CRISP-DM** framework.\n",
    "\n",
    "A pipeline is just an automated, sequential set of machine learning processes. The ordered stages of CRISP-DM (outside of possibly *Business Understanding*) and their underlying sub-processes compose the pipeline used for this example.\n",
    "\n",
    "## Business Understanding\n",
    "* What is the expected use and benefit of the model?\n",
    "    - The model's output will be used as input in *another* ML solution along with other data germane to the investment question. The model is expected to improve on the current approach and increase revenue. \n",
    "* What is the current/'baseline' approach?\n",
    "    - A team estimates median home value using complex rules with an error rate of +/-20%, on average\n",
    "* What type of problem is this?\n",
    "   - The Census data contains historical median home values by block group (i.e. the label or target) so it is a **Supervised Learning** problem\n",
    "   - Median home value is a *numerical* value so it is a **Regression** problem \n",
    "   - Data is *not* generated continuously, nor are investment decisions needed in real or near real-time so it is a **Batch Learning** problem\n",
    "* What performance metric will be used?\n",
    "    - **Root Mean Squared Error (RMSE)** will be used\n",
    "\n",
    "***\n",
    "\n",
    "## Exploratory Data Analysis and Preparation\n",
    "### Data Understanding\n",
    "Many stages in CRISP-DM require reporting after the completion of certain tasks. **Jupyter Notebooks** can be used for just such documentation so it's good to feel comfortable using them since they've become the defacto industry standard. \n",
    "\n",
    "The Data Understanding stage has many reporting instances since the tasks within drive the remainder of the project. *The algorithms used in machine learning are far less important than managing the data being modeled.*\n",
    "\n",
    "### Collect initial data\n",
    "Before gathering data into Python, it's typically wise to create an **Environment**. Environments isolate your Python configuration, package version, dependencies, etc. so that code doesn't 'break' when updates occur on a user's system. This isn't necessary for course labs, but it's very important when working on projects in the real world. Check out the `virtualenv` package for more details.\n",
    "\n",
    "Getting started, a function is created to pull the data for the rest of the project. The California Housing Data is stored on GitHub so there is no need to create a query and download it off the U.S. Census Bureau website. Another consideration, ETL preparation, is also unnecessary for this lab. As with most tools in machine learning, there are a variety of options to choose from both inside and outside of the Python environment. Check out [Airflow](https://github.com/apache/airflow) and [Prefect](https://www.prefect.io/) for a few options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for loading data\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "\n",
    "# Create a function for pulling U.S. Census data for California housing\n",
    "DOWNLOAD_ROOT = (\n",
    "    \"https://raw.githubusercontent.com/jsukup/Developing-an-ML-Model-End-to-End/master/\"\n",
    ")\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "\n",
    "fetch_housing_data()  # Pull data set from GitHub\n",
    "\n",
    "\n",
    "# Create a function for loading data into a Python object\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "housing.head()  # Inspect first five rows of data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lir6zz8huOM3"
   },
   "source": [
    "### Describe data\n",
    "The data set contains 10 columns -- nine features and one label, `median_house_value`. While Python has several built-in methods for exploring and describing a data set, the `ydata_profiling` package will provide a more robust report complete with both numerical and visual data descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ydata_profiling and inspect data file\n",
    "!pip install ydata-profiling\n",
    "from ydata_profiling import ProfileReport \n",
    "\n",
    "ProfileReport(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5-ctQynxvyu"
   },
   "source": [
    "The data set contains 20,640 observations (rows) which, by machine learning standards, is fairly small. Most of the features are numeric (7) with one categorical. The `ydata_profiling` package also indicates that some features might be highly correlated with one another and provides a warning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYYSECLn10Jd"
   },
   "source": [
    "### Verify data quality\n",
    "Each feature is also visualized with a histogram. Histograms are a way of visualizing the distributions of numeric features. Near the bottom of the report are two correlation visualizations (although both provide about the same info, *Spearman* is intended for *non-parametric* data -- data we *don't* assume to be normally distributed (i.e. following  the \"bell curve\" distribution). A correlation near \"1.00\" (i.e. the red cells) indicate two highly-correlated features.\n",
    "\n",
    "There are four things to note from examining the `ydata_profiling` report:\n",
    "\n",
    "1. The feature `median_income` does not look like it's expressed in USD. In the real-world, consulting with the SME would be a good idea to find out what's going on. In this example, it's determined that the feature was preprocessed in the Census data set and that the numbers represent, roughly, tens of thousands of dollars (i.e. \"3\" means \"~\\\\$30,000\"). Also, income values were capped at ~\"15\" or \"\\\\$150,000\" so any income higher than that was simply recorded as \"15.\"\n",
    "2. When features are capped at a maximum value (think about age ranges, for example, where typically \"65+\" covers anyone over 65) it's called *censoring*. Both `housing_median_age` and `median_house_value` appear to be capped. The later could be an issue since it's the label the model is going to try and predict. In the real-world, consultation with the end users (in this case, those making the investment decisions) would be necessary to see if this is a problem. Two options: collect the `median_house_values` elsewhere for capped districts or remove those observations entirely.\n",
    "3. Features are measured on different scales which will impact how machine learning algorithms estimate model parameters. More on that during the Data Preparation stage.\n",
    "4. Based on the histograms, many features look *tail heavy* meaning that they extend far to the right although the median value is far to the left. A lot of machine learning algorithms perform better (or even *assume*) that features are normally distributed so it could be a good idea to apply a *transformation* to these to obtain that.\n",
    "\n",
    "### Data Preparation\n",
    "The first task in Data Preparation is to split the data set into a *training* and a *testing* data set. Remember, this is so we can train a machine learning model and have something left over to evaluate how well it works on unseen data like it will encounter in the future. This is in this stage to avoid any *data snooping* -- where the data scientist might gleen insights into model development by looking at the data that might ultimately end up in the test set. The whole idea of the test set is to pretend it doesn't exist yet.\n",
    "\n",
    "Scikit-Learn provides a few functions for creating training and testing data sets including `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nl6oCDOC3t8"
   },
   "outputs": [],
   "source": [
    "# Create training/testing data sets with a random 80/20 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(\n",
    "    housing, test_size=0.2, random_state=734\n",
    ")  # Syncs random number\n",
    "\n",
    "test_set.head()  # Inspect first 5 rows of test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4o6MOzGDtgb"
   },
   "source": [
    "The `train_test_split` function creates the two data sets according to the arguments set by randomly selecting 20% (i.e. `test_set=0.2`) of the data for the test set. Setting `random_state=734` locks the random number generator so the same observations will be selected every time the code block is executed. If it wasn't included, a different selection would be made each time making it impossible for ourselves or other data scientists to reproduce it in the future. Not good!\n",
    "\n",
    "However, *generalization* requires consideration of the feature distributions in the real-world. The `test_train_split` function only splits the data randomly and doesn't try to *stratify* the split by features' real distributions. This means the data might not be *representative* of reality and the model won't generalize well.\n",
    "\n",
    "Suppose when talking to the SME that the `median_income` feature is considered a great predictor for `median_house_value`. Therefore, you want to ensure that its distribution is representative in both the training and testing data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVB7UyxmC3uB"
   },
   "outputs": [],
   "source": [
    "# Visualize median_income histogram\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "housing['median_income'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYsEjjheICus"
   },
   "source": [
    "Most `median_income` values range between ~\\\\$15,000 and ~\\\\$60,000 with some values in the right tail. One way to address this skew is to categorize ranges of income values in an attempt to make them closer to a bell curve distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzpY-EYgC3uD"
   },
   "outputs": [],
   "source": [
    "# Create income_cat feature with five strata for income ranges\n",
    "import numpy as np\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(\n",
    "    housing[\"median_income\"],\n",
    "    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5],\n",
    ")\n",
    "\n",
    "# Visualize income_cat histogram\n",
    "plt.figure(figsize=(15, 10))\n",
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx-ifWlwC3uI"
   },
   "source": [
    "Using Scikit-Learn's `StratifiedShuffleSplit` function, the training and testing data sets can be generated to ensure the `median_income` feature is representative in each split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGkhetHUC3uK"
   },
   "outputs": [],
   "source": [
    "# Generate stratified training and testing data sets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=734)\n",
    "\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw6lpafPM1Hm"
   },
   "source": [
    "In order to make sure the stratification and split worked correctly, a simple data frame can be constructed to compare the proportions of median incomes across the five categories between the random test split and the stratified test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrA_6mEXC3uQ"
   },
   "outputs": [],
   "source": [
    "# Create function for calculating category proportions\n",
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "\n",
    "# Generate random 80/20 training and testing data sets\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=734)\n",
    "\n",
    "# Generate data frame to compare each technique\n",
    "compare_props = pd.DataFrame(\n",
    "    {\n",
    "        \"Overall\": income_cat_proportions(housing),\n",
    "        \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "        \"Random\": income_cat_proportions(test_set),\n",
    "    }\n",
    ").sort_index()\n",
    "\n",
    "compare_props[\"Rand. %error\"] = (\n",
    "    100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    ")\n",
    "compare_props[\"Strat. %error\"] = (\n",
    "    100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    ")\n",
    "\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYS_zjOBPRdg"
   },
   "source": [
    "Although not *exactly* the same proportions, the stratified split is very close to the proportions in the full data set. The new `income_cat` feature will be deleted so the data sets are back to their original sets of features while still keeping stratification in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bk9DIIRHC3uV"
   },
   "outputs": [],
   "source": [
    "# Drop income_cat feature from both training and testing data sets\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_Ke2zT8C3uW"
   },
   "source": [
    "### Explore and visualize data\n",
    "Histograms are a good utility visualization but can be limited (and they don't work at all for categorical or geographic data). Since this machine learning project is based on real estate, it might be a good idea to explore the geographic data as it relates to the `median_house_value` label to see if anything interesting pops up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbGzz_bdC3ug"
   },
   "outputs": [],
   "source": [
    "# Plot districts by lat/long with median_house_price colored\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "housing = strat_train_set.copy()  # This just creates a copy of the training data\n",
    "california_img = mpimg.imread(\n",
    "    \"https://raw.githubusercontent.com/jsukup/handson-ml2/master/images/end_to_end_project/california.png\"\n",
    ")\n",
    "\n",
    "ax = housing.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    figsize=(15, 10),\n",
    "    s=housing[\"population\"] / 100,\n",
    "    label=\"Population\",\n",
    "    c=\"median_house_value\",\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    "    colorbar=False,\n",
    "    alpha=0.4,\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    california_img,\n",
    "    extent=[-124.55, -113.80, 32.45, 42.05],\n",
    "    alpha=0.5,\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticklabels([\"$%dk\" % (round(v / 1000)) for v in tick_values], fontsize=14)\n",
    "cbar.set_label(\"Median House Value\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtDef0fufyqu"
   },
   "source": [
    "### Feature engineering\n",
    "Identifying relationships between features and between features and the label is an important **Exploratory Data Analysis**, or **EDA**, task. Correlation statistics provide a measure of the strength of relationships between features and were already visualized earlier with the `ydata_profiling` package. Strong relationships between features and labels can indicate features important to model training later on. This may also help to eliminate features from the data that aren't helpful, thus reducing the size of our data set and potentially the computation time needed for model training. This is called **Feature Selection**.\n",
    "\n",
    "Feature Engineering also includes **Feature Extraction** where new features are created from existing ones. The data set contains three features that might be more useful if they were *household-level* measures rather than *district-level* -- `total_rooms`, `total_bedrooms`, and `population`. Creating and comparing the new features' correlations to `median_house_value` with the original features shows that all three have stronger relationships to the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fhxND4_C3uu"
   },
   "outputs": [],
   "source": [
    "# Create three new features for household-level measures\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"] / housing[\"households\"]\n",
    "\n",
    "# Show correlations\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJxPcpptC3u7"
   },
   "source": [
    "### Data cleaning\n",
    "Cleaning \"messy\" data is said to account for 80% of a data scientist's work load. This is not a very 'fun' part of machine learning solution building, so in the real-world it's advisable to build functions for these tasks and eventually create a repository you can use for future projects. Most of the cleaning steps for this data set are simple and won't require elaborate function coding.\n",
    "\n",
    "First, create a new training data set that removes the label since sometimes the transformations applied during this stage shouldn't be applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMWWpnP9C3u9"
   },
   "outputs": [],
   "source": [
    "# Create a new training data set and split labels into another object\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj9TDNJ5ywQI"
   },
   "source": [
    "Next, most machine learning algorithms won't work with missing data so they need to be addressed. There are multiple approaches to handling missing values including:\n",
    "* Drop observations (rows) from the data containing a missing value\n",
    "    - ```\n",
    "    housing.dropna(subset=['total_bedrooms']) \n",
    "      ```\n",
    "* Drop features (columns) from the data containing a missing value\n",
    "    - ```\n",
    "    housing.drop('total_bedrooms', axis=1)\n",
    "      ```\n",
    "* Impute missing values by setting them to some value like the feature mean, median, mode, etc.\n",
    "    - ```\n",
    "    median = housing['total_bedrooms'].median()\n",
    "    housing['total_bedrooms'].fillna(median, inplace=True)\n",
    "      ```\n",
    "\n",
    "Scikit-learn also has built-in functions for dealing with missing values. The function `SimpleImputer` will replace missing values according to the `strategy=` argument. Note, that it only works on numeric features so `ocean_proximity` will have to be treated differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlLI1lLcC3vK"
   },
   "outputs": [],
   "source": [
    "# Instantiate the SimpleImputer function and set strategy to 'median'\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Create new data set with ocean_proximity removed\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# Replace missing values with median\n",
    "X = imputer.fit_transform(housing_num)\n",
    "\n",
    "# Transform X from numpy array into data frame\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqxtEXnhC3vk"
   },
   "source": [
    "Before appending the `ocean_proximity` feature back to the data set, it needs to be transformed into a numeric feature. Computers operate with numbers so text is meaningless and needs to be converted to something it can read and understand when used in a machine learning algorithm. \n",
    "\n",
    "Categorical features can either be *nominal* or *ordinal*. Nominal features have no hierarchy meaning it can't be quantified how different the values are (e.g. hair colors). Ordinal features *do* have an inherent hierarchy (e.g. \"Good\", \"Better\", \"Best\"). It appears that `ocean_proximity` doesn't have a hierarchy so Scikit-Learn's `OneHotEncoder` can be used to create **Dummy Features**. Dummy Features are binary features added to a data set for each value a categorical feature can take. `ocean_proximity` can take on five values so five new categorical features are created with a \"1\" assigned to the observation's given value for the original categorical feature and a \"0\" for all others (see below).\n",
    "\n",
    "![](https://naadispeaks.files.wordpress.com/2018/04/mtimfxh.png?w=471&h=151)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bG0ktnMaC3vq"
   },
   "outputs": [],
   "source": [
    "# Create dummy features for ocean_proximity\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "\n",
    "# Print out category names and array\n",
    "print(cat_encoder.categories_)\n",
    "print(housing_cat_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OGDDahBC3vv"
   },
   "source": [
    "Pipelines in machine learning solutions can streamline the tasks performed up to this point an beyond. So far, all tasks have been performed piecemeal but Scikit-Learn provides several built-in tools for combining these operations into custom-made classes.\n",
    "\n",
    "Scikit-Learn's API is designed using consistent object characteristics:\n",
    "* *Estimators* are objects that can estimate (or *learn*) parameters from a data set. This is the goal of all machine learning algorithms -- learn the parameters of a model that best describe the relationship between features and (in Supervised Learning) labels. When the `.fit()` method is called, an estimator is being implemented.\n",
    "* *Transformers* are objects that transform a data set's values. Sometimes this transformation is based on the results of an estimator's learned parameters. When the `.transform()` method is called, the values in a data set are being transformed. Transformers also have a `.fit_transform()` method that applies both estimation and transformation sequentially.\n",
    "* *Predictors* take an existing model (typically learned with an estimator) and use it to make predictions with new data. When the `.predict()` method is called, a predictor is being used to estimate values using a model. They also have a `.score()` method for measuring the quality of predictions when compared to actual labels in a test set (for Supervised Learning problems).\n",
    "\n",
    "Below, a Python class is made called `CombinedAttrAdder` which calculates the two (or three depending on whether or not `add_bedrooms_per_room` is set to `True`) additional features created earlier to transform district-level features to household-level ones. It is then instantiated in the object `attr_adder` and applied (using the `.transform()` method) to the `housing` data set and saved in the `housing_extra_attribs` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcEhLrxPC3vw"
   },
   "outputs": [],
   "source": [
    "# Import base estimator and transformer functions\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Column indices in housing object for features needed\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "\n",
    "# Feature extraction\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True):  # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[\n",
    "                X, rooms_per_household, population_per_household, bedrooms_per_room\n",
    "            ]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "\n",
    "# Apply transformation to data.\n",
    "# If `add_bedrooms_per_room` set to 'False', remove 'bedrooms_per_room' column\n",
    "# from dataframe\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "# Create new dataframe with extracted features\n",
    "housing_extra_attribs = pd.DataFrame(\n",
    "    housing_extra_attribs,\n",
    "    columns=list(housing.columns)\n",
    "    + [\"rooms_per_household\", \"population_per_household\", \"bedrooms_per_room\"],\n",
    "    index=housing.index,\n",
    ")\n",
    "\n",
    "# Inspect top five rows of new dataframe\n",
    "housing_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcX4NnHdC3vz"
   },
   "source": [
    "### Feature scaling\n",
    "Measurement scales across features in a data set are often different. Most machine learning algorithms do not perform well when this is the case since they rely on calculating *distances* between observations for estimating similarity or difference. The California Housing Data set is no exception. For example, the range of values for `total_rooms` is from 6 to 39,320 while the range for `median_income` is just 0 to 15 (note that label values are generally never scaled). \n",
    "\n",
    "Two approaches for handling feature scaling are **Normalization** (or **Min-Max Scaling**) and **Standardization**. Normalization scales features to fall withing the range 0 and 1 according to the following:\n",
    "\n",
    "$$\\LARGE\n",
    "x'=\\frac{x_i-X_{min}}{X_{max}-X_{min}}\n",
    "$$\n",
    "\n",
    "Standardization transforms features to have a mean of 0 and a standard deviation of 1, essentially giving it a normal distribution. Normally distributed features usually 'play well' with machine learning algorithms. Unlike Normalization, feature values are not constrained between 0 and 1. Standardization is also less influenced by outliers compared to Normalization. Standardization can be formulated as:\n",
    "\n",
    "$$\\LARGE\n",
    "x'=\\frac{x_i-X_\\mu}{X_\\sigma}\n",
    "$$\n",
    "\n",
    "Scikit-Learn provides built-in functions for both scaling techniques. `StandardScaler` applies standardization which is the approach applied below within the context of a simple Pipeline created using the Scikit-Learn `Pipeline` function. This Pipeline applies imputation of missing values, feature extraction using the class created earlier, and Standardization. This Pipeline only applies to *numeric* features in the data set so `ocean_proximity` will need to be addressed separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpuxpVBMC3vz"
   },
   "outputs": [],
   "source": [
    "# Create pipeline for numeric feature preparation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"attribs_adder\", CombinedAttributesAdder()),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pass numeric features through pipeline\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wupo4jLcBPH3"
   },
   "source": [
    "Pipelines in Scikit-Learn take a list of name/estimator pairs that define the sequence of data preparation tasks. All but the final estimator needs to have a `transform()` method as well. When executing a pipeline the `.fit_transform()` method is automatically called for all intermediary estimators before passing the output to the next estimator.\n",
    "\n",
    "For convenience, it makes sense to combine pipelines for handling *both* numeric and categorical features together. The Scikit-Learn class `ColumnTransformer` allows for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtIsmik1C3v1"
   },
   "outputs": [],
   "source": [
    "# Create combined numeric and categorical pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Extract names of numeric and categorical columns\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer(\n",
    "    [(\"num\", num_pipeline, num_attribs), (\"cat\", OneHotEncoder(), cat_attribs)]\n",
    ")\n",
    "\n",
    "# Pass all features through pipeline\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared.shape  # Rows x columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efTspXzOEbYd"
   },
   "source": [
    "`ColumnTransformer` works by taking the names of numeric feature columns and applying the first pipeline to just those columns. Then it applies the one transformation we did previously to the `ocean_proximity` feature, `OneHotEncoding`. When applied, the final prepared data set contains 16512 rows and 16 columns. \n",
    "****\n",
    "# END OF DAY 1\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "In this stage a model is trained on the training data to learn the optimal parameters that best describe the relationship between features and labels. Noted earlier, it typically only accounts for ~20% of the data scientist's time on a project but that could fluctuate based on the complexity of the problem or if a custom model needs to be trained.\n",
    "\n",
    "### Train model on training data\n",
    "First, we'll train a linear regression model. Training most models using Scikit-Learn only requires two lines of code if all the model's hyperparameters are left at their defaults (which typically are good enough for initial model training and evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0r0W8hd0ZVu"
   },
   "outputs": [],
   "source": [
    "# Train a linear regression model on preprocessed housing data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Test model predictions with performance metric\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "\n",
    "# Just for fun, also check MAE\n",
    "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
    "f\"The Root Mean Squared Error (RMSE) is: {lin_rmse}; The Mean Absolute Error (MAE) is: {lin_mae}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-jz9B6WC3wL"
   },
   "source": [
    "Comparing the model's predictions to the actual values shows that it isn't working all that well -- in some cases being off by hundreds of thousands of dollars. Both the RMSE and MAE errors are fairly substantial and probably wouldn't be acceptable in a real-world application for the real estate company's investment team.\n",
    "\n",
    "Since this model was tested using the *training data* set (i.e. the same data used to train the model), these poor performance scores indicate that the model is **Underfitting** -- the opposite of **Overfitting** -- meaning the model is not complex enough to capture the underlying relationship between the features and labels.\n",
    "\n",
    "A different model, a **Decision Tree** is trained below (this model hasn't been discussed yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwnbPiNkC3wU"
   },
   "outputs": [],
   "source": [
    "# Train a decision tree model on preprocessed housing data\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=734)\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Test model predictions with performance metric\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
    "f\"The Root Mean Squared Error (RMSE) is: {tree_rmse}; The Mean Absolute Error (MAE) is: {tree_mae}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcjl8oJ-3uZG"
   },
   "source": [
    "WOW! A model with zero error? How is that possible? It's much more likely that the model is horribly **overfitting** the data (remember, the data being used to *test* the model's performance is the same data used to *train* it). \n",
    "\n",
    "### Evaluation using cross-validation\n",
    "Rather than use the training data for both training and testing, **cross-validation** is a better approach for estimating model performance. The `test_train_split` function has already been showcased but oftentimes **K-fold cross-validation** is a better alternative. The way it works is by first splitting the data set into \"K\" number of partitions, or \"folds.\" Then it trains and evaluates a model K-times picking a different fold for evaluation each time while training the remaining K-folds. So, if \"10\" is chosen for the value of \"K,\" 10 folds will be created and 10 models will be trained/evaluated. Then, the average is taken across each performance measure (i.e., RMSE). This is a method for *smoothing out* the performance metric estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k16FfCp1C3wY"
   },
   "outputs": [],
   "source": [
    "# Cross-validate the decision tree model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(\n",
    "    tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10\n",
    ")  # Select 10 folds\n",
    "\n",
    "# Calculate RMSE across scores\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "\n",
    "# Print scores\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZswYDvY7UFl"
   },
   "source": [
    "According to the cross-validation scores, on average the model's predictions are off by ~\\$69,000 which is probably not acceptable. This also confirms that the decision tree model is most likely overfitting on the training data. Compare to linear regression cross-validation performance metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sACAiwd5C3wa"
   },
   "outputs": [],
   "source": [
    "# Cross-validate the linear regression model\n",
    "lin_scores = cross_val_score(\n",
    "    lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10\n",
    ")\n",
    "\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phwIJyuuC3wb"
   },
   "source": [
    "Also an unacceptable performance score. Thankfully, there are numerous models available (and *tuning techniques*) for training better models than these two. **Random Forest** is like building multiple, but slightly different, decision trees and is what is referred to as an **Ensemble Method** because it aggregates predictions across multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rcrhxtAC3wb"
   },
   "outputs": [],
   "source": [
    "# Train a random forest model on preprocessed housing data\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=734)\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Test model predictions with performance metric\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "\n",
    "f\"The Root Mean Squared Error (RMSE) is: {forest_rmse}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hd-3G5n_o75"
   },
   "source": [
    "Substantially better performance than a single Decision Tree model. Using cross-validation the performance estimate should be more precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtyXqym0C3wd"
   },
   "outputs": [],
   "source": [
    "# Cross-validate the Random Forest model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest_scores = cross_val_score(\n",
    "    forest_reg,\n",
    "    housing_prepared,\n",
    "    housing_labels,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFrPKQ5p5Lc8"
   },
   "source": [
    "The Random Forest model still performs better than both linear regression and a Decision Tree, but the average error after cross-validation is fairly large. \n",
    "\n",
    "### Create shortlist of candidate models\n",
    "Remember, when building machine learning solutions there is no \"magic bullet\" in terms of best-performing models. It's never known which machine learning algorithm is going to work best. This is called the **No Free Lunch Theory** and is the rationale behind training and choosing several candidate models before selecting the best one. For the sake of time, the Random Forest model will be the only one considered. \n",
    "\n",
    "### Hyperparameter optimization\n",
    "In machine learning, algorithms attempt to find the best parameters of a model (basically a mathematical formalization, or equation) that captures the relationship between features and their labels. **Hyperparameters**, or *tuning parameters* on the other hand, tell the model *how* to train. This typically applies constraints to model training that can both improve predictive performance and limit overfitting.\n",
    "\n",
    "One way to find a model's optimal hyperparameters would be to manually train several models with different hyperparameters and see which one gives the best performance scores. Yet this could become quickly inefficient as the number of hyperparameters available to try becomes large. **Grid Search** is one approach for handling hyperparameter optimization. Grid Search works by being provided a set of possible hyperparameters to try, then builds an exhaustive set of models using every combination of hyperparameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioi8ZCoeC3wh"
   },
   "outputs": [],
   "source": [
    "# Train a Random Forest model using grid search with cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a list of hyperparameter values set as key:value pairs\n",
    "param_grid = [\n",
    "    # Try 12 (3×4) combinations of hyperparameters\n",
    "    {\"n_estimators\": [3, 10, 30], \"max_features\": [2, 4, 6, 8]},\n",
    "    # Try 6 (2×3) combinations with bootstrap set as False\n",
    "    {\"bootstrap\": [False], \"n_estimators\": [3, 10], \"max_features\": [2, 3, 4]},\n",
    "]\n",
    "\n",
    "# Train Random Forest model\n",
    "forest_reg = RandomForestRegressor(random_state=734)\n",
    "\n",
    "# Five fold cross-validation: (12+6)*5=90 rounds of training\n",
    "grid_search = GridSearchCV(\n",
    "    forest_reg,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "# Fit model to training data\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Print best parameters and estimator\n",
    "f\"Optimal estimated hyperparameters: {grid_search.best_estimator_}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkilXUesC3wi"
   },
   "source": [
    "Even with just one base model, Random Forest (and not even playing with all available hyperparameters for Random Forest), 90 different models are trained. The resulting best model (i.e., the model with the lowest MSE) has `max_features=6`, and `n_estimators=30`. Look at the score of each hyperparameter combination tested during the Grid Search on the top five performing models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YNkWlQyC3wl"
   },
   "outputs": [],
   "source": [
    "# Inspect metrics for top 5 models and sort best to worst\n",
    "pd.DataFrame(grid_search.cv_results_).sort_values([\"rank_test_score\"], ascending=True)[\n",
    "    :5\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JajhemfeUiKQ"
   },
   "source": [
    "**Random Search** is another technique for finding the best hyperparameters of a model. Unlike grid search, random search does *not* try exhaustive combinations of hyperparameters but instead, as the name implies, tries randomized combinations (rather than setting an explicit set of hyperparameter values, a *range* is given for the algorithm to choose from). This works well when large numbers of hyperparameters are being tuned and can provide comparable results to grid search as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jayEGwjKC3wn"
   },
   "outputs": [],
   "source": [
    "# Train a Random Forest model using random search with cross-validation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Create a range of hyperparameter values set as key:value pairs\n",
    "param_distribs = {\n",
    "    \"n_estimators\": randint(low=1, high=200),\n",
    "    \"max_features\": randint(low=1, high=8),\n",
    "}\n",
    "# Train Random Forest model\n",
    "forest_reg = RandomForestRegressor(random_state=734)\n",
    "\n",
    "# Five fold cross-validation: 10 iterations using random hyperparameters\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    forest_reg,\n",
    "    param_distributions=param_distribs,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    random_state=734,\n",
    ")\n",
    "\n",
    "# Fit model to training data\n",
    "rnd_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Print best parameters and estimator\n",
    "f\"Optimal estimated hyperparameters: {rnd_search.best_estimator_}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvvWj_vbXi0u"
   },
   "outputs": [],
   "source": [
    "# Inspect metrics for top 5 models and sort best to worst\n",
    "pd.DataFrame(rnd_search.cv_results_).sort_values([\"rank_test_score\"], ascending=True)[\n",
    "    :5\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-XyPdKDaFjB"
   },
   "source": [
    "### Ensemble methods\n",
    "Another way to fine-tune models during model selection is to combine the predictions of several best-performing models. **Ensemble Methods** do just this and Random Forest is an example of this technique. Ensembles tend to smooth out prediction estimations similar to getting a second test or second opinion when going to the doctor. Ensemble methods won't be covered in detail in this course.\n",
    "\n",
    "Before going further, Random Forest can provide information on which features contribute the most to accurate predictions. This information can also be useful for determining if any features can be dropped if they are not that helpful in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mg59xh0UC3wp"
   },
   "outputs": [],
   "source": [
    "# Print feature important scores\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "features = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "pd.DataFrame(\n",
    "    sorted(zip(feature_importances, features), reverse=True),\n",
    "    columns=[\"Feature Importance\", \"Feature\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nQFgP_0fX5H"
   },
   "source": [
    "### Test set evaluation\n",
    "When satisfied with what is set to be the final model, the test set is used to evaluate. Remember, the test set hasn't been used at all in the training of the model. This is to *simulate* the model's performance on data it will encounter in its production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpX2rz_9C3wr"
   },
   "outputs": [],
   "source": [
    "# Test final model on test set\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)  # Convert to Root Mean Squared Error\n",
    "f\"Final Model RMSE: {final_rmse}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM63N9QMC3wt"
   },
   "source": [
    "Performance is typically slightly worse on the test set than what was measured during cross-validation. However, the data scientist should resist the temptation of going back and tuning hyperparameters after seeing the test set performance as this will inevitably start to overfit the model and start to train it on the idiosyncracies of the test data.\n",
    "\n",
    "## Deployment\n",
    "At this point, model training and testing is complete! But this is really only the beginning -- there is no such thing as calling the machine learning solution 'done.' Next is the Deployment stage which typically requires the data scientist to exhaustively report on the model training process (i.e. lessons learned, what worked/didn't work, assumptions made, system limitations, etc.). Again, Jupyter Notebooks are very helpful and widely-used for these purposes. \n",
    "\n",
    "The data scientist (or team of data scientists) typically need to *sell* their solution to executives and decision makers prior to productionizing. With approval to launch, the data scientist will typically start to pass off the solution to a **Machine Learning Engineer** and **MLOps Engineer** responsible for deployment to the end-user. They are tasked with determining how best to deliver the model with considerations on how it will be hosted (i.e. cloud or on-prem) and associated costs in addition to how to keep its predictions optimal over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congats! That's the end of the lab!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "The Machine Learning Pipeline.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
