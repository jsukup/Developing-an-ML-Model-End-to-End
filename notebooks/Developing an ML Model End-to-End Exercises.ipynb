{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"RmozR9UHC3w6"},"source":["# Developing an ML Model End-to-End: Exercises\n","\n","Now that you've had some exposure to using Python for training, tuning, evaluating, and selecting an ML model, it's time to try some exercises on your own. Below, you'll find 5 exercises that ask you to continue your work with the California Housing Data Set used in the Developing an ML Model End-to-End lab. Each exercise will begin with some starter code to help guide your development towards a solution to the problem. Remember, there isn't just one solution to the problem and your code may produce the similar results even if it's written differently from what is supplised in the solution notebook.\n","\n","If you haven't already downloaded the California Housing Data Set, the code block below is from the course lab. You can execute it again to download the data set to Anaconda Notebooks or your local computer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import packages for loading data\n","import os\n","import tarfile\n","from six.moves import urllib\n","import pandas as pd\n","\n","# Create a function for pulling U.S. Census data for California housing\n","DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/jsukup/handson-ml2/master/'\n","HOUSING_PATH = os.path.join('datasets', 'housing')\n","HOUSING_URL = DOWNLOAD_ROOT + 'datasets/housing/housing.tgz'\n","\n","def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n","    if not os.path.isdir(housing_path):\n","        os.makedirs(housing_path)\n","    tgz_path = os.path.join(housing_path, 'housing.tgz')\n","    urllib.request.urlretrieve(housing_url, tgz_path)\n","    housing_tgz = tarfile.open(tgz_path)\n","    housing_tgz.extractall(path=housing_path)\n","    housing_tgz.close()\n","\n","fetch_housing_data() # Pull data set from GitHub\n","\n","# Create a function for loading data into a Python object\n","def load_housing_data(housing_path=HOUSING_PATH):\n","    csv_path = os.path.join(housing_path, 'housing.csv')\n","    return pd.read_csv(csv_path)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Q1: \n","Try training a **Support Vector Machine** regressor (`sklearn.svm.SVR`), with various Hyperparameters such as `kernel='linear'` (with 3 values for the `C` hyperparameter) and `kernel='rbf'` (with 3 values for the `C` and `gamma` Hyperparameters). Don't worry about what these Hyperparameters mean for now. \n","\n","How does the best `SVR` predictor perform? Print the best model's RMSE and parameters.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZBOlXV5mC3w7"},"outputs":[],"source":["# Train a Support Vector Machine using GridSearchCV from the Scikit Learn library to find the best parameters\n","from sklearn.svm import SVR\n","from sklearn.model_selection import GridSearchCV\n","\n","# Create a list of hyperparameter values set as key:value pairs\n","param_grid = [\n","        {'kernel': ['linear'], 'C': [10., 3000., 30000.0]},\n","        {'kernel': ['rbf'], 'C': [1.0, 30., 1000.0],\n","         'gamma': [0.01, 0.3, 3.0]},\n","]\n","\n","# Train Support Vector Machine model\n","\n","\n","### YOUR CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"HYoo7y_RC3w_"},"source":["### Q2:\n","Try replacing `GridSearchCV` with `RandomizedSearchCV` and set `n_iter=10`  and `cv=5`. Print the best model's RMSE and parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aT3E15HoC3xA"},"outputs":[],"source":["# Try random search with Support Vector Machine model\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import expon, reciprocal\n","\n","# see https://docs.scipy.org/doc/scipy/reference/stats.html\n","# for `expon()` and `reciprocal()` documentation and more probability \n","# distribution functions.\n","\n","# Create a range of hyperparameter values set as key:value pairs\n","# Note: gamma is ignored when kernel is 'linear'\n","param_distribs = {\n","        'kernel': ['linear', 'rbf'],\n","        'C': reciprocal(20, 200000),\n","        'gamma': expon(scale=1.0),\n","}\n","### YOUR CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"P7l6FM_4C3xG"},"source":["### Q3:\n","Try creating a transformer class called `TopFeatureSelector` with three methods: `__init__`, `fit`, and `transform` that selects the top features based on their importances and add it to the existing `full_pipeline` created earlier. Both the `BaseEstimator` and `TransformerMixin` classes used earlier will be appropriate for creating the new top features class. When done, the new Pipeline will look something like this:\n","\n","```\n","pipeline = Pipeline([\n","                     ('preparation', full_pipeline),\n","                     ('feature_selection', TopFeatureSelector(feature_importances, k))\n","])\n","```\n","\n","**NOTE**: This feature selector assumes that you have already computed the feature importances somehow (for example using a `RandomForestRegressor`). Though tempting to compute them directly in the `TopFeatureSelector`'s `fit()` method, this would likely slow down Hyperparameter optimization since the feature importances would have to be computed for every Hyperparameter combination (unless you implement some sort of cache)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qU0HqGEhC3xH"},"outputs":[],"source":["# Import estimators, pipeline, and Random Forest\n","import numpy as np\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","\n","# Use RandomForestRegressor to compute feature importance scores\n","### YOUR CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"yyIOC9LaC3xc"},"source":["### Q4:\n","Try creating a single Pipeline that does the full data preprocessing plus the final prediction. This can be accomplished using the Pipeline created in Q3 and adding one final step to the list for the model estimation. Try using the SVR model used earlier where Random Search found the best model parameters (i.e. `rnd_search_SVR.best_params_`) as the final step.\n","\n","Try the Pipeline on the first 5 observations from the training data by printing out the model's prediction and the actual label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9QCc2JWC3xc"},"outputs":[],"source":["# Import pipeline and Support Vector Machine packages\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import SVR\n","import pandas as pd\n","\n","# Create final pipeline for preprocessing and prediction\n","final_pipeline = Pipeline([\n","    ('preparation', full_pipeline),\n","    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n","    ('svm_reg', SVR(**rnd_search_SVR.best_params_))\n","])\n","### YOUR CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"ZT5LO3P2C3xg"},"source":["### Q5:\n","Automatically explore some preprocessing options using `GridSearchCV` on the `final_pipeline` just created. Print out the parameters of the best model\n","\n","To access Hyperparameters nested within other Pipelines, double underscores (i.e. \"\\__\") can be used. For example, to tune the value of `k` in the `TopFeatureSelector` class, set the Pipeline 'key:value' pair to `'feature_selector__k'`followed by some method for generating values for `k`. \n","\n","Double underscores allow access to nested tasks within the Pipeline. Another example: to access the `SimpleImputer` nested all the way down in the first Pipeline created for numeric features alone (i.e. `num_pipeline`), use the names given to each Pipeline object in sequence:\n","\n","```\n","'preparation__num__imputer__strategy'\n","```\n","where:\n","```\n","preparation__ = [name for `full_pipeline` in `final_pipeline` object]\n","num__ = [name for `num_pipeline` in `full_pipeline` object]\n","imputer__ = [name for `SimpleImputer` in `num_pipeline` object]\n","strategy = [hyperparameter for `SimpleImputer`]\n","```\n","Finish by printing the parameters of the best model found.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zq3FSC91C3xg"},"outputs":[],"source":["# Create grid search parameters for pipeline\n","param_grid = [{\n","    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n","    'feature_selection__k': list(range(1, len(feature_importances) + 1))\n","}]\n","\n","# Five fold cross-validation: (3*16)*5=240 rounds of training  \n","### YOUR CODE HERE ###"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"name":"The Machine Learning Pipeline.ipynb","provenance":[]},"file_extension":".py","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"nbformat":4,"nbformat_minor":0}
